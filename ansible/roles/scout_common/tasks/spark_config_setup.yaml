---
# Create Spark configuration ConfigMap
# This is used by JupyterHub and other Spark-enabled services
#
# Parameters:
#   spark_hive_metastore_uri: Which Hive metastore to connect to
#     - Pass hive_metastore_endpoint for write access (HL7 transformer, extractors)
#     - Pass hive_metastore_endpoint_readonly for readonly access (Jupyter)
#     - Defaults to hive_metastore_endpoint (write) for backward compatibility

- name: Create Spark ConfigMap
  vars:
    spark_defaults_lines: >-
      {{
        (s3_manage_secrets
          | ternary([
              'spark.hadoop.fs.s3a.access.key ' ~ (spark_s3_username | default(s3_lake_reader)),
              'spark.hadoop.fs.s3a.secret.key ' ~ (spark_s3_password | default(s3_lake_reader_secret))
            ], [])
        )
        + (
          ((s3_endpoint | default('')) | length > 0)
          | ternary(['spark.hadoop.fs.s3a.endpoint ' ~ s3_endpoint], [])
        )
        + [
          'spark.hadoop.fs.s3a.endpoint.region ' ~ (s3_region | default('us-east-1')),
          'spark.databricks.delta.schema.autoMerge.enabled true',
          'spark.databricks.delta.merge.repartitionBeforeWrite.enabled true',
          'spark.databricks.delta.constraints.allowUnenforcedNotNull.enabled true',
          'spark.sql.extensions io.delta.sql.DeltaSparkSessionExtension',
          'spark.sql.catalog.spark_catalog org.apache.spark.sql.delta.catalog.DeltaCatalog',
          'spark.hadoop.fs.s3a.path.style.access ' ~ (s3_force_path_style | ternary('true', 'false')),
          'spark.hadoop.hive.metastore.uris ' ~ (spark_hive_metastore_uri | default(hive_metastore_endpoint)),
          'spark.sql.warehouse.dir ' ~ delta_lake_path,
          'spark.driver.extraJavaOptions -Divy.cache.dir=/tmp -Divy.home=/tmp',
          'spark.executor.memory ' ~ (spark_memory | default('1G')),
          'spark.driver.memory ' ~ (spark_memory | default('1G'))
        ]
        + (s3_manage_secrets
          | ternary(['spark.hadoop.fs.s3a.aws.credentials.provider org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider'],
                    ['spark.hadoop.fs.s3a.aws.credentials.provider com.amazonaws.auth.WebIdentityTokenCredentialsProvider']))
      }}
  kubernetes.core.k8s:
    state: present
    definition:
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: '{{ spark_defaults_configmap_name }}'
        namespace: '{{ spark_defaults_configmap_namespace }}'
      data:
        spark-defaults.conf: "{{ spark_defaults_lines | join('\n') }}"
